"""
T√ºrk√ße PDF QA Sistemi - Ana QA Sƒ±nƒ±fƒ±
"""
import torch
import faiss
import numpy as np
from typing import List
from transformers import AutoTokenizer, AutoModelForCausalLM
from sentence_transformers import SentenceTransformer
from config import Config
from pdf_processor import PDFProcessor

class TurkishPDFQA:
    """T√ºrk√ße PDF Soru-Cevap Ana Sƒ±nƒ±fƒ±"""
    
    def __init__(self):
        """Sƒ±nƒ±fƒ± ba≈ülatƒ±r ve modelleri y√ºkler"""
        print("üöÄ T√ºrk√ße PDF QA sistemi ba≈ülatƒ±lƒ±yor...")
        
        # Torch optimizasyonu
        torch.set_float32_matmul_precision('high')
        
        # LLM modeli y√ºkleniyor
        print(f"ü§ñ LLM modeli y√ºkleniyor: {Config.LLM_MODEL_NAME}")
        self.tokenizer = AutoTokenizer.from_pretrained(Config.LLM_MODEL_NAME)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
        self.model = AutoModelForCausalLM.from_pretrained(
            Config.LLM_MODEL_NAME,
            torch_dtype=Config.TORCH_DTYPE,
            device_map=Config.DEVICE_MAP,
            low_cpu_mem_usage=True
        )
        self.device = next(self.model.parameters()).device
        print(f"‚úÖ LLM modeli y√ºklendi (Device: {self.device})")
        
        # Embedding modeli y√ºkleniyor
        print(f"üß† Embedding modeli y√ºkleniyor: {Config.EMBEDDING_MODEL_NAME}")
        self.embed_model = SentenceTransformer(Config.EMBEDDING_MODEL_NAME)
        print("‚úÖ Embedding modeli y√ºklendi")
        
        # PDF processor
        self.pdf_processor = PDFProcessor(self.tokenizer)
        
        # Veri saklama
        self.pdf_chunks = []
        self.embeddings = None
        self.index = None
        
        print("‚úÖ Sistem hazƒ±r!")
    
    def load_pdfs(self, pdf_files: List[str]):
        """PDF dosyalarƒ±nƒ± y√ºkler ve index olu≈üturur"""
        print("\n" + Config.SEPARATOR_LINE)
        print("üìö PDF Y√úKLEME VE ƒ∞NDEXLEME")
        print(Config.SEPARATOR_LINE)
        
        try:
            # PDF'leri i≈üle
            self.pdf_chunks = self.pdf_processor.process_pdf_files(pdf_files)
            
            # Embeddings olu≈ütur
            print("üß† Embedding'ler olu≈üturuluyor...")
            self.embeddings = self.embed_model.encode(
                self.pdf_chunks, 
                convert_to_numpy=True, 
                show_progress_bar=True
            )
            
            # FAISS index olu≈ütur
            print("üîç Arama index'i olu≈üturuluyor...")
            self.index = faiss.IndexFlatL2(self.embeddings.shape[1])
            self.index.add(self.embeddings)
            
            print(f"‚úÖ ƒ∞ndexleme tamamlandƒ±!")
            print(f"   üìä Toplam chunk sayƒ±sƒ±: {len(self.pdf_chunks)}")
            print(f"   üéØ Embedding boyutu: {self.embeddings.shape[1]}")
            
        except Exception as e:
            print(f"‚ùå PDF y√ºkleme hatasƒ±: {str(e)}")
            raise
    
    def ask_question(self, question: str, top_k: int = None) -> str:
        """Soruya cevap verir"""
        if not self.is_ready():
            raise ValueError("Sistem hazƒ±r deƒüil! √ñnce PDF dosyalarƒ±nƒ± y√ºkleyin.")
        
        if top_k is None:
            top_k = Config.DEFAULT_TOP_K
        
        try:
            # Soru embedding'i
            question_embedding = self.embed_model.encode([question], convert_to_numpy=True)
            
            # En yakƒ±n chunk'larƒ± bul
            D, I = self.index.search(question_embedding, top_k)
            top_chunks = [self.pdf_chunks[i] for i in I[0]]
            
            # Her chunk i√ßin cevap √ºret
            chunk_answers = []
            for i, chunk in enumerate(top_chunks):
                print(f"üîÑ Chunk {i+1}/{len(top_chunks)} i≈üleniyor...")
                answer = self._generate_answer_for_chunk(chunk, question)
                chunk_answers.append(answer)
            
            # Cevaplarƒ± birle≈ütir
            final_answer = self._fuse_answers(chunk_answers, question)
            return final_answer
            
        except Exception as e:
            print(f"‚ùå Soru cevaplama hatasƒ±: {str(e)}")
            raise
    
    def _generate_answer_for_chunk(self, chunk: str, question: str) -> str:
        """Tek chunk i√ßin cevap √ºretir"""
        prompt = f"""Metin: {chunk}\n\nSoru: {question}\n\nCevap:"""
        
        inputs = self.tokenizer(
            prompt, 
            return_tensors="pt", 
            truncation=True, 
            padding=True
        ).to(self.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=Config.MAX_NEW_TOKENS_CHUNK,
                temperature=Config.TEMPERATURE,
                top_p=Config.TOP_P,
                top_k=Config.TOP_K,
                do_sample=False,
                repetition_penalty=Config.REPETITION_PENALTY,
                no_repeat_ngram_size=Config.NO_REPEAT_NGRAM_SIZE,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return response.strip()
    
    def _fuse_answers(self, answers: List[str], question: str) -> str:
        """Birden fazla cevabƒ± birle≈ütirir"""
        fusion_prompt = "\n".join([f"Par√ßa {i+1} Cevap: {ans}" for i, ans in enumerate(answers)])
        fusion_prompt += f"""

Sadece yukarƒ±daki cevaplara dayalƒ± teknik ve doƒüru bir T√ºrk√ße cevap ver.
Genel a√ßƒ±klamalardan, tahminlerden ve konu dƒ±≈üƒ± ifadelerden ka√ßƒ±n. Sadece doƒürudan sorunun teknik cevabƒ±nƒ± ver.

SORU: {question}

CEVAP:"""
        
        fusion_inputs = self.tokenizer(
            fusion_prompt, 
            return_tensors="pt", 
            truncation=True, 
            padding=True
        ).to(self.device)
        
        with torch.no_grad():
            final_output = self.model.generate(
                **fusion_inputs,
                max_new_tokens=Config.MAX_NEW_TOKENS_FINAL,
                temperature=Config.TEMPERATURE,
                top_p=Config.TOP_P,
                top_k=Config.TOP_K,
                do_sample=False,
                repetition_penalty=Config.REPETITION_PENALTY,
                no_repeat_ngram_size=Config.NO_REPEAT_NGRAM_SIZE,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )
        
        final_answer = self.tokenizer.decode(final_output[0], skip_special_tokens=True)
        return final_answer.split("CEVAP:")[-1].strip()
    
    def is_ready(self) -> bool:
        """Sistemin hazƒ±r olup olmadƒ±ƒüƒ±nƒ± kontrol eder"""
        return (self.pdf_chunks is not None and 
                len(self.pdf_chunks) > 0 and 
                self.embeddings is not None and 
                self.index is not None)
    
    def get_stats(self) -> dict:
        """Sistem istatistiklerini d√∂nd√ºr√ºr"""
        if not self.is_ready():
            return {"status": "not_ready"}
        
        return {
            "status": "ready",
            "chunk_count": len(self.pdf_chunks),
            "embedding_dim": self.embeddings.shape[1],
            "device": str(self.device),
            "model_name": Config.LLM_MODEL_NAME
        } 